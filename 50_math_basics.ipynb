{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "080e93e4",
      "metadata": {
        "id": "080e93e4"
      },
      "source": [
        "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8120a37f",
      "metadata": {
        "id": "8120a37f"
      },
      "source": [
        "# Mathematics Basics\n",
        "\n",
        "**With `NumPy, pandas & PyTables`**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0806b7ff",
      "metadata": {
        "id": "0806b7ff"
      },
      "source": [
        "&copy; Dr. Yves J. Hilpisch | The Python Quants GmbH\n",
        "\n",
        "http://tpq.io | [training@tpq.io](mailto:trainin@tpq.io) | [@dyjh](http://twitter.com/dyjh)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aabc89f0",
      "metadata": {
        "id": "aabc89f0"
      },
      "source": [
        "## Python and Big Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390dec04",
      "metadata": {
        "id": "390dec04"
      },
      "source": [
        "Python per se is _not_ a Big Data technology. However, Python in combination with packages like `pandas` or `PyTables` allows the management and the analysis of quite large data sets.\n",
        "\n",
        "For our purposes, we define Big Data as a (number of) **object(s)** and/or **data file(s)** that do(es) _not_ fit into the memory of a single computer (server, node, etc.) &mdash; whatever hardware you are using for data analytics. On such a data file, typical analytics and computational tasks, like counting, aggregation and selection shall be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e61c47",
      "metadata": {
        "id": "c8e61c47"
      },
      "source": [
        "## Large Scale Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f62c1343",
      "metadata": {
        "id": "f62c1343"
      },
      "source": [
        "Computation = Mathematics + Programming + Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ebdf03b",
      "metadata": {
        "id": "7ebdf03b"
      },
      "source": [
        "Large Scale Computation = Mathematics + Programming + Large Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed864f54",
      "metadata": {
        "id": "ed864f54"
      },
      "source": [
        "## Out-of-Memory Analytics with NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d4ef45",
      "metadata": {
        "tags": [],
        "id": "a1d4ef45"
      },
      "source": [
        "Sometimes operations on `NumPy ndarray` objects generate so many temporary objects that the available memory does not suffice to finish the desired operation. An example might be `a.dot(a.T)`, i.e. the dot product of an array `a` with iteself transposed.\n",
        "\n",
        "Such an operation needs memory for **three arrays**: `a`, `a.T` and `a.dot(a.T)`. If the array `a` is sufficiently large, say 50% of the free memory, such an operation is impossible with the usual approach.\n",
        "\n",
        "A solution is to work with **disk-based arrays** and to use **memory maps** of these arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15330d01",
      "metadata": {
        "id": "15330d01"
      },
      "source": [
        "Some **imports** first and a check of the **free memory**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcI10xltQxi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!git clone https://github.com/tpq-classes/mathematics_basics.git\n",
        "import sys\n",
        "sys.path.append('mathematics_basics')\n"
      ],
      "id": "LXcI10xltQxi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd602e7",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4dd602e7"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc1fa28",
      "metadata": {
        "id": "bbc1fa28"
      },
      "outputs": [],
      "source": [
        "psutil.virtual_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d6ffd5",
      "metadata": {
        "id": "22d6ffd5"
      },
      "outputs": [],
      "source": [
        "print('RAM % used:', psutil.virtual_memory()[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c88f32a",
      "metadata": {
        "id": "1c88f32a"
      },
      "source": [
        "### Sample Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eee3fb8",
      "metadata": {
        "id": "3eee3fb8"
      },
      "source": [
        "We generate a larger `NumPy ndarray` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133a2a2b",
      "metadata": {
        "id": "133a2a2b"
      },
      "outputs": [],
      "source": [
        "from numpy.random import default_rng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4cf7424",
      "metadata": {
        "id": "a4cf7424"
      },
      "outputs": [],
      "source": [
        "rng = default_rng(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40300ca4",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "40300ca4"
      },
      "outputs": [],
      "source": [
        "m = 10000\n",
        "n = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e716bfc8",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "e716bfc8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "a = rng.standard_normal((m, n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3478dbcb",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3478dbcb"
      },
      "outputs": [],
      "source": [
        "a.nbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5f5965",
      "metadata": {
        "id": "db5f5965"
      },
      "source": [
        "Checking **memory** again &ndash; and that the object (reference pointer) indeed **owns the data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd86993e",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cd86993e"
      },
      "outputs": [],
      "source": [
        "psutil.virtual_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44fa67fb",
      "metadata": {
        "tags": [],
        "id": "44fa67fb"
      },
      "outputs": [],
      "source": [
        "a.flags.owndata\n",
        "  # the object owns the in-memory data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85e31a9",
      "metadata": {
        "id": "e85e31a9"
      },
      "source": [
        "Simple **operations** on the in-memory `ndarray` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e67322",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b2e67322"
      },
      "outputs": [],
      "source": [
        "a[:3, :3]\n",
        "  # sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b252ed",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "92b252ed"
      },
      "outputs": [],
      "source": [
        "%time a.mean()\n",
        "  # reductions work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4dcbdbb",
      "metadata": {
        "id": "e4dcbdbb"
      },
      "source": [
        "Now save this object **to disk** ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55e7672",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "a55e7672"
      },
      "outputs": [],
      "source": [
        " path = '/content/'  # needs to be adjusted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a095f326",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "a095f326"
      },
      "outputs": [],
      "source": [
        "%time np.save(path + 'od', a)\n",
        "  # save memory array to disk (SSD)\n",
        "  # (can need less time than in-memory generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed78e4b",
      "metadata": {
        "id": "eed78e4b"
      },
      "source": [
        "... and **delete** the in-memory object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256bb2bc",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "256bb2bc"
      },
      "outputs": [],
      "source": [
        "del a\n",
        "  # delete the in-memory version\n",
        "  # to free memory -- somehow ...\n",
        "  # gc does not work \"instantly\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf78dbfc",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cf78dbfc"
      },
      "outputs": [],
      "source": [
        "psutil.virtual_memory()\n",
        "  # garbage collection does not bring that much ...\n",
        "  # memory usage has not changed significantly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188873d8",
      "metadata": {
        "id": "188873d8"
      },
      "source": [
        "### Memory Map of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a1a5e8",
      "metadata": {
        "id": "a7a1a5e8"
      },
      "source": [
        "Using the saved object, we generate a new `memmap` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f1f1cb",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "65f1f1cb"
      },
      "outputs": [],
      "source": [
        "od = np.lib.format.open_memmap(path + 'od.npy', dtype=np.float64, mode='r')\n",
        "  # open memmap array with the array file as data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74861505",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "74861505"
      },
      "outputs": [],
      "source": [
        "od.flags.owndata\n",
        "  # object does not own the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc86b27",
      "metadata": {
        "id": "dcc86b27"
      },
      "source": [
        "It mainly behaves the **same way** as in-memory `ndarray` objects behave."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a5ff9c",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b6a5ff9c"
      },
      "outputs": [],
      "source": [
        "od[:3, :3]\n",
        "  # compare sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73903ebe",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "73903ebe"
      },
      "outputs": [],
      "source": [
        "%time od.mean()\n",
        "  # operations in NumPy as usual\n",
        "  # somewhat slower of course ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bf6bd8",
      "metadata": {
        "id": "a8bf6bd8"
      },
      "source": [
        "### Memory Maps of (Intermediate) Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431284aa",
      "metadata": {
        "id": "431284aa"
      },
      "source": [
        "Major memory problems with `NumPy ndarray` objects generally arise due to **temporary arrays** needed to store intermediate results. We therefore generate `memmap` objects to store intermediate and final results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fcd5556",
      "metadata": {
        "id": "0fcd5556"
      },
      "source": [
        "First, for the **transpose of the array**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a40bef4",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3a40bef4"
      },
      "outputs": [],
      "source": [
        "tr = np.memmap(path + 'tr.npy', dtype=np.float64, mode='w+', shape=(n, m))\n",
        "  # memmap object for transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d9d136",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "21d9d136"
      },
      "outputs": [],
      "source": [
        "%time tr[:] = od.T[:]\n",
        "  # write transpose to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9654429",
      "metadata": {
        "id": "b9654429"
      },
      "outputs": [],
      "source": [
        "!ls -n $path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a024f536",
      "metadata": {
        "id": "a024f536"
      },
      "source": [
        "Second, for the **final results**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8fab29",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8d8fab29"
      },
      "outputs": [],
      "source": [
        "re = np.memmap(path + 're.npy', dtype=np.float64, mode='w+', shape=(m, m))\n",
        "  # memmap object for result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824595be",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "824595be"
      },
      "outputs": [],
      "source": [
        "%time re[:] = od.dot(tr)[:]\n",
        "  # store results on disk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2422aed3",
      "metadata": {
        "id": "2422aed3"
      },
      "source": [
        "### Final Look and Cleaning Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cc28e1",
      "metadata": {
        "id": "59cc28e1"
      },
      "source": [
        "Lots of data (`od + tr + re`) has been crunched/created without a real memory burden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b7d0dd",
      "metadata": {
        "id": "41b7d0dd"
      },
      "outputs": [],
      "source": [
        "psutil.virtual_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6615100",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b6615100"
      },
      "outputs": [],
      "source": [
        "!ls -n $path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f65b29",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "90f65b29"
      },
      "outputs": [],
      "source": [
        "!rm $path*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f75236ba",
      "metadata": {
        "id": "f75236ba"
      },
      "source": [
        "### Using a Sub-Process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d01841",
      "metadata": {
        "id": "48d01841"
      },
      "source": [
        "The `concurrent` module allows the use of a **separate sub-process** for callables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be810c2",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3be810c2"
      },
      "outputs": [],
      "source": [
        "import concurrent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80aec22e",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "80aec22e"
      },
      "outputs": [],
      "source": [
        "def generate_array_on_disk(m, n):\n",
        "    # memory inefficient operation\n",
        "    a = rng.standard_normal((m, n))\n",
        "    np.save(path + 'od.npy', a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee16872",
      "metadata": {
        "id": "6ee16872"
      },
      "source": [
        "The use of such a sub-process makes sure that any memory used by the sub-process gets immediately freed after the sub-process is terminated. This leaves the **free memory of the current process** mainly unchanged. Avoids \"unpredictable\" behaviour of `Python` garbage collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c8cc63",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "15c8cc63"
      },
      "outputs": [],
      "source": [
        "psutil.virtual_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c1227f",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "c6c1227f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "with concurrent.futures.ThreadPoolExecutor() as subprocess:\n",
        "    subprocess.submit(generate_array_on_disk, m, n).result()\n",
        "  # separate sub-process is started, the callable is executed\n",
        "  # the process with all its memory usage is killed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f6a309",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "17f6a309"
      },
      "outputs": [],
      "source": [
        "psutil.virtual_memory()\n",
        "  # meanwhile memory was freed again"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3a4bb7",
      "metadata": {
        "id": "2c3a4bb7"
      },
      "source": [
        "Final look and clean-up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722670fe",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "722670fe"
      },
      "outputs": [],
      "source": [
        "!ls -n $path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880a2529",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "880a2529"
      },
      "outputs": [],
      "source": [
        "!rm $path*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b822e532",
      "metadata": {
        "id": "b822e532"
      },
      "source": [
        "## Processing (Too) Large CSV Files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "747730bf",
      "metadata": {
        "id": "747730bf"
      },
      "source": [
        "We generate a CSV file on disk that is **too large** to fit into memory. We process this file with the help of `pandas` and `PyTables`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0121b6e",
      "metadata": {
        "id": "c0121b6e"
      },
      "source": [
        "First, some imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a343011",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0a343011"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b781775",
      "metadata": {
        "id": "9b781775"
      },
      "source": [
        "### Generating an Example CSV File"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b120e7",
      "metadata": {
        "id": "e1b120e7"
      },
      "source": [
        "Number of **rows** to be generated for random data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3857a7d0",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3857a7d0"
      },
      "outputs": [],
      "source": [
        "N = int(1e5)\n",
        "N"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c99ea9",
      "metadata": {
        "id": "58c99ea9"
      },
      "source": [
        "Using both random **integers** as well as **floats**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81afbbf2",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "81afbbf2"
      },
      "outputs": [],
      "source": [
        "ran_int = rng.integers(0, 10000, size=(2, N))\n",
        "ran_flo = rng.standard_normal((2, N))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c356d14e",
      "metadata": {
        "id": "c356d14e"
      },
      "source": [
        "Filename for **`csv` file**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99a79bed",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "99a79bed"
      },
      "outputs": [],
      "source": [
        "csv_name = path + 'data.csv'\n",
        "csv_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b337601",
      "metadata": {
        "id": "0b337601"
      },
      "source": [
        "**Writing the data** row by row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9bc734",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ad9bc734"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "with open(csv_name, 'w') as csv_file:\n",
        "    header = 'date,int1,int2,flo1,flo2\\n'\n",
        "    csv_file.write(header)\n",
        "    for _ in range(20):\n",
        "        # 20 times the original data set\n",
        "        for i in range(N):\n",
        "            row = '%s,%i,%i,%f,%f\\n' % \\\n",
        "                    (dt.datetime.now(), ran_int[0, i], ran_int[1, i],\n",
        "                                    ran_flo[0, i], ran_flo[1, i])\n",
        "            csv_file.write(row)\n",
        "        print('Size on disk:', os.path.getsize(csv_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db525e5b",
      "metadata": {
        "id": "db525e5b"
      },
      "source": [
        "**Excursion**: If only the numerical data is to be written to disk, using `np.savetext()` can be more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d1a586",
      "metadata": {
        "id": "58d1a586"
      },
      "outputs": [],
      "source": [
        "ran = np.vstack((ran_int, ran_flo)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9cbb1e",
      "metadata": {
        "id": "fd9cbb1e"
      },
      "outputs": [],
      "source": [
        "ran[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc271af",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0fc271af"
      },
      "outputs": [],
      "source": [
        "csv_name_ = path + 'data_.csv'\n",
        "csv_name_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5ea1d9",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4e5ea1d9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "np.savetxt(csv_name_, ran, delimiter=',')  # just a single data set (not 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203da878",
      "metadata": {
        "id": "203da878"
      },
      "source": [
        "**Delete** the original `NumPy ndarray` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf07f8c",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1bf07f8c"
      },
      "outputs": [],
      "source": [
        "del ran\n",
        "del ran_int\n",
        "del ran_flo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154ff1a9",
      "metadata": {
        "id": "154ff1a9"
      },
      "source": [
        "**Reading some rows** to check the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3672da",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4e3672da"
      },
      "outputs": [],
      "source": [
        "with open(csv_name, 'r') as csv_file:\n",
        "    for _ in range(5):\n",
        "        print(csv_file.readline(), end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2dbf9d",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8d2dbf9d"
      },
      "outputs": [],
      "source": [
        "#with open(csv_name_, 'r') as csv_file:\n",
        "#    for _ in range(5):\n",
        "#        print(csv_file.readline(), end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec53d41",
      "metadata": {
        "id": "0ec53d41"
      },
      "source": [
        "### Reading and Writing with pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206bc1df",
      "metadata": {
        "id": "206bc1df"
      },
      "source": [
        "The filename for the `pandas HDFStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4d5afe",
      "metadata": {
        "id": "0c4d5afe"
      },
      "outputs": [],
      "source": [
        "!ls -n $path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa2a6ef",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "7aa2a6ef"
      },
      "outputs": [],
      "source": [
        "pd_name = path + 'data.h5p'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72453a6b",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "72453a6b"
      },
      "outputs": [],
      "source": [
        "h5 = pd.HDFStore(pd_name, 'w')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2f1dea0",
      "metadata": {
        "id": "d2f1dea0"
      },
      "source": [
        "`pandas` allows to read data from (large) files chunk-wise via a **file-iterator**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67bde8ed",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "67bde8ed"
      },
      "outputs": [],
      "source": [
        "it = pd.read_csv(csv_name, iterator=True, chunksize=N / 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d78a6bf8",
      "metadata": {
        "id": "d78a6bf8"
      },
      "source": [
        "Reading and storing the data **chunk-wise**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98dbbc0",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "f98dbbc0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i, chunk in enumerate(it):\n",
        "    h5.append('data', chunk)\n",
        "    if i % 20 == 0:\n",
        "        print('Size on disk:', os.path.getsize(pd_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64062c77",
      "metadata": {
        "id": "64062c77"
      },
      "source": [
        "The resulting `HDF5` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e378ea",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "04e378ea"
      },
      "outputs": [],
      "source": [
        "print(h5.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c3ff0d0",
      "metadata": {
        "id": "0c3ff0d0"
      },
      "source": [
        "### Disk-Based Analytics with pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c7901c",
      "metadata": {
        "id": "90c7901c"
      },
      "source": [
        "The **disk-based** `pandas DataFrame` mainly behaves like an **in-memory** object &ndash; but these operations are not memory efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0531241",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "d0531241"
      },
      "outputs": [],
      "source": [
        "%time h5['data'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2be0d8",
      "metadata": {
        "id": "9f2be0d8"
      },
      "source": [
        "**Data selection and plotting** works as with regular `pandas DataFrame` objects &ndash; again not really memory efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db72299b",
      "metadata": {
        "id": "db72299b"
      },
      "outputs": [],
      "source": [
        "from pylab import plt\n",
        "plt.style.use('seaborn-v0_8')\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0599670a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0599670a"
      },
      "outputs": [],
      "source": [
        "%time h5['data']['flo2'][0:N:1000].cumsum().plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c0658a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "92c0658a"
      },
      "outputs": [],
      "source": [
        "h5.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b95b4bba",
      "metadata": {
        "id": "b95b4bba"
      },
      "source": [
        "The major reason is that the `DataFrame` **data structure is broken up** (e.g. columns) during storage. For analytics it has to be put together in-memory again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d2e9df",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "65d2e9df"
      },
      "outputs": [],
      "source": [
        "import tables as tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6483f777",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6483f777"
      },
      "outputs": [],
      "source": [
        "h5 = tb.open_file(path + 'data.h5p', 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf2ed78",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1bf2ed78"
      },
      "outputs": [],
      "source": [
        "h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4932f63d",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4932f63d"
      },
      "outputs": [],
      "source": [
        "h5.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acef2a56",
      "metadata": {
        "id": "acef2a56"
      },
      "source": [
        "### Reading with pandas and Writing with PyTables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838b3b02",
      "metadata": {
        "id": "838b3b02"
      },
      "source": [
        "The `PyTables` database file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305eb988",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "305eb988"
      },
      "outputs": [],
      "source": [
        "import tables as tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1f05da",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2c1f05da"
      },
      "outputs": [],
      "source": [
        "tb_name = path + 'data.h5t'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c66dfb4",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4c66dfb4"
      },
      "outputs": [],
      "source": [
        "h5 = tb.open_file(tb_name, 'w')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0758c9",
      "metadata": {
        "id": "ae0758c9"
      },
      "source": [
        "Using a **`rec array` object** of `NumPy` to provide the row description for the `PyTables` table. To this end, a **custom `dtype` object** is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5aa2daf",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "a5aa2daf"
      },
      "outputs": [],
      "source": [
        "dty = np.dtype([('date', 'S26'), ('int1', '<i8'), ('int2', '<i8'),\n",
        "                                 ('flo1', '<f8'), ('flo2', '<f8')])\n",
        "  # change dtype for date from object to string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2deefd",
      "metadata": {
        "id": "cf2deefd"
      },
      "source": [
        "Adding **compression** to the mix (less storage, better backups, better data transfer, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074bce4b",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "074bce4b"
      },
      "outputs": [],
      "source": [
        "filters = tb.Filters(complevel=2, complib='blosc')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa4768e",
      "metadata": {
        "id": "4aa4768e"
      },
      "source": [
        "Again **reading and writing chunk-wise**, this time appending to a `PyTables table` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593539cf",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "593539cf"
      },
      "outputs": [],
      "source": [
        "it = pd.read_csv(csv_name, iterator=True, chunksize=N / 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dce54ee",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "6dce54ee"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "tab = h5.create_table('/', 'data',\n",
        "            np.array(it.read().to_records(index=False),\n",
        "                     dty), filters=filters)\n",
        "  # initialize table object by using first chunk and adjusted dtype\n",
        "for chunk in it:\n",
        "    tab.append(chunk.to_records(index=False))\n",
        "tab.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effe6bcf",
      "metadata": {
        "id": "effe6bcf"
      },
      "source": [
        "The resulting `table` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7b2b5a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "4f7b2b5a"
      },
      "outputs": [],
      "source": [
        "h5.get_filesize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3b8b73",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "dd3b8b73"
      },
      "outputs": [],
      "source": [
        "tab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9543bd6",
      "metadata": {
        "id": "b9543bd6"
      },
      "source": [
        "### Out-of-Memory Analytics with PyTables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9982361",
      "metadata": {
        "id": "a9982361"
      },
      "source": [
        "**Data on disk** can be used as if it would be both _in-memory_ and _uncompressed_. De-compression is done at run-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8605f424",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8605f424"
      },
      "outputs": [],
      "source": [
        "tab[N:N + 3]\n",
        "  # slicing row-wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17827e76",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "17827e76"
      },
      "outputs": [],
      "source": [
        "tab[N:N + 3]['date']\n",
        "  # access selected data points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78914c98",
      "metadata": {
        "id": "78914c98"
      },
      "source": [
        "**Counting** of rows is easily accomplished (although here not really needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2ecc4f",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2e2ecc4f"
      },
      "outputs": [],
      "source": [
        "%time len(tab[:]['flo1'])\n",
        "  # length of column (object)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f787bdff",
      "metadata": {
        "id": "f787bdff"
      },
      "source": [
        "**Aggregation** operations, like summing up or calculating the mean value, are another application area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e3dc7a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "e2e3dc7a"
      },
      "outputs": [],
      "source": [
        "%time tab[:]['flo1'].sum()\n",
        "  # sum over column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40122fe",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "f40122fe"
      },
      "outputs": [],
      "source": [
        "%time tab[:]['flo1'].mean()\n",
        "  # mean over column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1cab7cc",
      "metadata": {
        "id": "d1cab7cc"
      },
      "source": [
        "Typical, `SQL`-like, **conditions and queries** can be added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc2c135",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2cc2c135"
      },
      "outputs": [],
      "source": [
        "%time sum([row['flo2'] for row in tab.where('(flo1 > 3) & (int2 < 1000)')])\n",
        "  # sum combined with condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f071b64",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8f071b64"
      },
      "outputs": [],
      "source": [
        "h5.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2024487a",
      "metadata": {
        "id": "2024487a"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587d8cd5",
      "metadata": {
        "id": "587d8cd5"
      },
      "source": [
        "All operations have been on data sets that do not fit (if uncompressed) into the memory of the machine they haven been implemented on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "373fad06",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "373fad06"
      },
      "outputs": [],
      "source": [
        "!ls -n $path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88080cd4",
      "metadata": {
        "id": "88080cd4"
      },
      "source": [
        "Using compression of course reduces the size of the `PyTables table` object relative to the `csv` and the `pandas HDFStore` files. This might, in certain circumstances, lead to file sizes that would again fit in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a77ed7",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "79a77ed7"
      },
      "outputs": [],
      "source": [
        "!rm $path*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7395f818",
      "metadata": {
        "tags": [],
        "id": "7395f818"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac74d9f",
      "metadata": {
        "id": "eac74d9f"
      },
      "source": [
        "`Python` and packages like **`NumPy, pandas, PyTables`** provide useful means and approaches to circumvent the limitations of free memory on a single computer (node, server, etc.).\n",
        "\n",
        "Key to the **performance of such out-of-memory operations** are mainly the storage hardware (speed/capacity), the data format used (e.g. `HDF5` vs.  relational databases) and in some scenarios also the use of performant compression algorithms.\n",
        "\n",
        "Reading writing speed of **`SSD` hardware** is evolving fast:\n",
        "\n",
        "* status quo: **3+GB/s** reading/writing (e.g. MacBook 2020)\n",
        "* available: **6+GB/s** reading/writing (e.g. latest SSDs 2021)\n",
        "\n",
        "Check out [Fastest SSD Drives](https://www.gamingpcbuilder.com/ssd-ranking-the-fastest-solid-state-drives/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec33f564",
      "metadata": {
        "id": "ec33f564"
      },
      "source": [
        "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
        "\n",
        "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:training@tpq.io\">training@tpq.io</a>"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}